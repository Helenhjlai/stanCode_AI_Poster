{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j62kqrS88d5W",
        "outputId": "c7d2e935-7558-487a-d056-8e2df1054e56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Colab Notebooks\n"
          ]
        }
      ],
      "source": [
        "# Mount Google dirve\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Define Project Folder\n",
        "FOLDERNAME = 'Colab\\ Notebooks'\n",
        "\n",
        "%cd drive/MyDrive/$FOLDERNAME"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define device\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "print('Device:', device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E14LfCsO8idJ",
        "outputId": "b915ac3c-48f9-4331-8799-b37239b60a8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# download MNIST dataset --> (1, 28*28)\n",
        "# create mini batch meanwhile\n",
        "mnist_transform = transforms.Compose([transforms.ToTensor()])\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('./r_mnist_test', train=False, download=True, transform=mnist_transform),\n",
        "        batch_size=10, shuffle=True)\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('./r_mnist_train', train=True, download=True, transform=mnist_transform),\n",
        "        batch_size=10, shuffle=True)\n",
        "\n",
        "# hyperparameter\n",
        "batch_size = 10\n",
        "epoch = 1"
      ],
      "metadata": {
        "id": "QbWsWXHZ8kXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LeNet Model definition\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 300)\n",
        "        self.fc2 = nn.Linear(300, 100)\n",
        "        self.fc3 = nn.Linear(100, 10)\n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Vap1nIZ08tzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training model\n",
        "def train(model,optimizer):\n",
        "  for i in range(epoch):\n",
        "    for j,(data,target) in tqdm(enumerate(train_loader)):\n",
        "      model.train()\n",
        "      data = data.to(device)\n",
        "      target = target.to(device)\n",
        "      # print(data.shape)\n",
        "      # print(target.shape)\n",
        "      logit = model(data)\n",
        "      loss = F.cross_entropy(logit,target)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      if j % 1000 == 0:\n",
        "        print ('第{}筆資料，loss值等於{}'.format(j,loss))\n",
        "\n",
        "# validation process\n",
        "def test(model,name):\n",
        "  model.eval()\n",
        "  correct_num = torch.tensor(0).to(device)\n",
        "  with torch.no_grad():\n",
        "    for j,(data,target) in tqdm(enumerate(test_loader)):\n",
        "      data = data.to(device)\n",
        "      target = target.to(device)\n",
        "      logit = model(data)\n",
        "      pred = logit.max(1)[1]\n",
        "      # acc = pred.eq(target).sum().item() / 10000\n",
        "      num = torch.sum(pred==target)\n",
        "      correct_num = correct_num + num\n",
        "    print (correct_num)\n",
        "    # print ('\\n{} correct rate is {}'.format(name, acc))\n",
        "    print ('\\n{} correct rate is {}'.format(name, correct_num / 10000))"
      ],
      "metadata": {
        "id": "RyQ2s4p98wag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# calculate jacobian matrix for forward deviation\n",
        "def compute_jacobian(model, input):\n",
        "    var_input = input.clone()\n",
        "\n",
        "    var_input.detach_()\n",
        "    var_input.requires_grad = True\n",
        "    output = model(var_input)\n",
        "\n",
        "    num_features = int(np.prod(var_input.shape[1:]))\n",
        "    jacobian = torch.zeros([output.size()[1], num_features])\n",
        "    for i in range(output.size()[1]):\n",
        "        # zero_gradients(input)\n",
        "        if var_input.grad is not None:\n",
        "          var_input.grad.zero_()\n",
        "        # output.backward(mask,retain_graph=True)\n",
        "        output[0][i].backward(retain_graph=True)\n",
        "        # copy the derivative to the target place\n",
        "        jacobian[i] = var_input.grad.squeeze().view(-1, num_features).clone()\n",
        "\n",
        "    return jacobian.to(device)"
      ],
      "metadata": {
        "id": "OMCMsCqC88_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saliency map calcuation\n",
        "\n",
        "def saliency_map(jacobian, target_index, increasing, search_space, nb_features):\n",
        "    domain = torch.eq(search_space, 1).float()  # The search domain\n",
        "    # the sum of all features' derivative with respect to each class\n",
        "    all_sum = torch.sum(jacobian, dim=0, keepdim=True)\n",
        "    target_grad = jacobian[target_index]  # The forward derivative of the target class\n",
        "    others_grad = all_sum - target_grad  # The sum of forward derivative of other classes\n",
        "\n",
        "    # this list blanks out those that are not in the search domain\n",
        "    if increasing:\n",
        "        increase_coef = 2 * (torch.eq(domain, 0)).float().to(device)\n",
        "    else:\n",
        "        increase_coef = -1 * 2 * (torch.eq(domain, 0)).float().to(device)\n",
        "    increase_coef = increase_coef.view(-1, nb_features)\n",
        "\n",
        "    # calculate sum of target forward derivative of any 2 features.\n",
        "    target_tmp = target_grad.clone()\n",
        "    target_tmp -= increase_coef * torch.max(torch.abs(target_grad))\n",
        "    alpha = target_tmp.view(-1, 1, nb_features) + target_tmp.view(-1, nb_features, 1)  # PyTorch will automatically extend the dimensions\n",
        "\n",
        "    # calculate sum of other forward derivative of any 2 features.\n",
        "    others_tmp = others_grad.clone()\n",
        "    others_tmp += increase_coef * torch.max(torch.abs(others_grad))\n",
        "    beta = others_tmp.view(-1, 1, nb_features) + others_tmp.view(-1, nb_features, 1)\n",
        "\n",
        "    # zero out the situation where a feature sums with itself\n",
        "    tmp = np.ones((nb_features, nb_features), int)\n",
        "    np.fill_diagonal(tmp, 0)\n",
        "    zero_diagonal = torch.from_numpy(tmp).byte().to(device)\n",
        "\n",
        "    # According to the definition of saliency map in the paper (formulas 8 and 9),\n",
        "    # those elements in the saliency map that doesn't satisfy the requirement will be blanked out.\n",
        "    if increasing:\n",
        "        mask1 = torch.gt(alpha, 0.0)\n",
        "        mask2 = torch.lt(beta, 0.0)\n",
        "    else:\n",
        "        mask1 = torch.lt(alpha, 0.0)\n",
        "        mask2 = torch.gt(beta, 0.0)\n",
        "    # apply the mask to the saliency map\n",
        "    mask = torch.mul(torch.mul(mask1, mask2), zero_diagonal.view_as(mask1))\n",
        "    # do the multiplication according to formula 10 in the paper\n",
        "    saliency_map = torch.mul(torch.mul(alpha, torch.abs(beta)), mask.float())\n",
        "    # get the most significant two pixels\n",
        "    max_value, max_idx = torch.max(saliency_map.view(-1, nb_features * nb_features), dim=1)\n",
        "    p = max_idx // nb_features\n",
        "    q = max_idx % nb_features\n",
        "    return p, q, saliency_map"
      ],
      "metadata": {
        "id": "xxLyPdFx8_xd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perturbation_single(image, ys_target, theta, gamma, model):\n",
        "\n",
        "    copy_sample = np.copy(image)\n",
        "    var_sample =Variable(torch.from_numpy(copy_sample), requires_grad=True).to(device)\n",
        "\n",
        "    # outputs = model(var_sample)\n",
        "    # predicted = torch.max(outputs.data, 1)[1]\n",
        "    # print('测试样本扰动前的预测值：{}'.format(predicted[0]))\n",
        "\n",
        "    var_target = Variable(torch.LongTensor([ys_target,])).to(device)\n",
        "\n",
        "    if theta > 0:\n",
        "        increasing = True\n",
        "    else:\n",
        "        increasing = False\n",
        "\n",
        "    num_features = int(np.prod(copy_sample.shape[1:]))\n",
        "    shape = var_sample.size()\n",
        "\n",
        "    # perturb two pixels in one iteration, thus max_iters is divided by 2.0\n",
        "    max_iters = int(np.ceil(num_features * gamma / 2.0))\n",
        "\n",
        "    # masked search domain, if the pixel has already reached the top or bottom, we don't bother to modify it.\n",
        "    if increasing:\n",
        "        search_domain = torch.lt(var_sample, 0.99) #逐一元素比較var_sample和0.99\n",
        "    else:\n",
        "        search_domain = torch.gt(var_sample, 0.01)\n",
        "    search_domain = search_domain.view(num_features)\n",
        "\n",
        "    model.eval().to(device)\n",
        "    output = model(var_sample)\n",
        "    current = torch.max(output.data, 1)[1].cpu().numpy()\n",
        "    saliency = torch.zeros((1, 784, 784)).to(device)\n",
        "\n",
        "    iter = 0\n",
        "    while (iter < max_iters) and (current[0] != ys_target) and (search_domain.sum() != 0):\n",
        "        # calculate Jacobian matrix of forward derivative\n",
        "        jacobian = compute_jacobian(model, var_sample)\n",
        "        # get the saliency map and calculate the two pixels that have the greatest influence\n",
        "        p1, p2, s = saliency_map(jacobian, var_target, increasing, search_domain, num_features)\n",
        "        saliency += s\n",
        "        # apply modifications\n",
        "        var_sample_flatten = var_sample.view(-1, num_features).clone().detach_()\n",
        "        var_sample_flatten[0, p1] += theta\n",
        "        var_sample_flatten[0, p2] += theta\n",
        "\n",
        "        new_sample = torch.clamp(var_sample_flatten, min=0.0, max=1.0)\n",
        "        new_sample = new_sample.view(shape)\n",
        "        search_domain[p1] = 0\n",
        "        search_domain[p2] = 0\n",
        "        var_sample = Variable(torch.tensor(new_sample), requires_grad=True ).to(device)\n",
        "\n",
        "        output = model(var_sample)\n",
        "        current = torch.max(output.data, 1)[1].cpu().numpy()\n",
        "        iter += 1\n",
        "\n",
        "    adv_samples = var_sample.data.cpu().numpy()\n",
        "    return adv_samples, saliency"
      ],
      "metadata": {
        "id": "u6lExdGe9BQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# adversarial examples\n",
        "model_adv_filter = Net().to(device)"
      ],
      "metadata": {
        "id": "IHabVWs89C_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer\n",
        "optimizer2 = torch.optim.Adam(model_adv_filter.parameters())"
      ],
      "metadata": {
        "id": "V0Raw3Vl9HIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(model_adv_filter,optimizer2)\n",
        "test(model_adv_filter, 'adv_classifer')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6GDsgNK9JSJ",
        "outputId": "463074de-4ffe-4bb4-d5b6-0f12ecbc4ce4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "36it [00:03, 14.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "第0筆資料，loss值等於2.299304723739624\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1045it [00:06, 336.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "第1000筆資料，loss值等於0.0342840813100338\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2044it [00:09, 345.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "第2000筆資料，loss值等於0.021585002541542053\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "3045it [00:12, 267.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "第3000筆資料，loss值等於0.27309170365333557\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "4065it [00:16, 318.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "第4000筆資料，loss值等於0.012569770216941833\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "5052it [00:19, 346.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "第5000筆資料，loss值等於0.16115450859069824\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "6000it [00:21, 273.28it/s]\n",
            "1000it [00:01, 550.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(9672, device='cuda:0')\n",
            "\n",
            "adv_classifer correct rate is 0.967199981212616\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# randomly choose 5000 clean data and gen 5000 adversarial examples from train_loader\n",
        "# hyperparameter setting\n",
        "adver_nums_filter = 5000\n",
        "y_adv_target = 8\n",
        "theta = 1.0\n",
        "gamma = 0.1\n",
        "\n",
        "adver_example_by_JSMA_f = torch.zeros((batch_size,1,28,28)).to(device)\n",
        "adver_target_f = torch.zeros(5000).to(device)\n",
        "clean_example_f = torch.zeros((batch_size,1,28,28)).to(device)\n",
        "clean_target_f = torch.ones(5000).to(device)\n",
        "\n",
        "for i,(data,target) in enumerate(train_loader):\n",
        "  if i >= adver_nums_filter/batch_size :\n",
        "    break\n",
        "  if i == 0:\n",
        "    clean_example_f = data\n",
        "  else:\n",
        "    clean_example_f = torch.cat((clean_example_f,data),dim = 0)\n",
        "\n",
        "  cur_adver_example_by_JSMA_f = torch.zeros_like(data).to(device)\n",
        "\n",
        "  for j in range(batch_size):\n",
        "\n",
        "    pert_image_f, _ = perturbation_single(data[j].resize_(1,28*28).numpy(),y_adv_target,theta,gamma,model_adv_filter)\n",
        "    cur_adver_example_by_JSMA_f[j] = torch.from_numpy(pert_image_f).resize_(1, 28, 28).to(device)\n",
        "\n",
        "  #\n",
        "  if i == 0:\n",
        "    adver_example_by_JSMA_f = cur_adver_example_by_JSMA_f\n",
        "  else:\n",
        "    adver_example_by_JSMA_f = torch.cat((adver_example_by_JSMA_f , cur_adver_example_by_JSMA_f), dim = 0)\n",
        "\n",
        "print (adver_example_by_JSMA_f.shape)\n",
        "# print (adver_target)\n",
        "print (clean_example_f.shape)\n",
        "# print (clean_target)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJb7THpA9LqV",
        "outputId": "4555b576-a587-47a5-cbe8-25e52e28a11e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-3d63487e6aaa>:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  var_sample = Variable(torch.tensor(new_sample), requires_grad=True ).to(device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5000, 1, 28, 28])\n",
            "torch.Size([5000, 1, 28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# randomly choose 1000 clean data and gen 1000 adversarial examples from test_loader\n",
        "# hyperparameter setting\n",
        "adver_nums_filter_t = 1000\n",
        "y_adv_target = 8\n",
        "theta = 1.0\n",
        "gamma = 0.1\n",
        "\n",
        "adver_example_by_JSMA_t = torch.zeros((batch_size,1,28,28)).to(device)\n",
        "adver_target_t = torch.zeros(1000) .to(device)\n",
        "clean_example_t = torch.zeros((batch_size,1,28,28)).to(device)\n",
        "clean_target_t = torch.ones(1000).to(device)\n",
        "\n",
        "for i,(data,target) in enumerate(test_loader):\n",
        "  if i >= adver_nums_filter_t/batch_size :\n",
        "    break\n",
        "  if i == 0:\n",
        "    clean_example_t = data\n",
        "  else:\n",
        "    clean_example_t = torch.cat((clean_example_t,data),dim = 0)\n",
        "\n",
        "  cur_adver_example_by_JSMA_t = torch.zeros_like(data).to(device)\n",
        "\n",
        "  for j in range(batch_size):\n",
        "\n",
        "    pert_image_t, _ = perturbation_single(data[j].resize_(1,28*28).numpy(),y_adv_target,theta,gamma,model_adv_filter)\n",
        "    cur_adver_example_by_JSMA_t[j] = torch.from_numpy(pert_image_t).resize_(1, 28, 28).to(device)\n",
        "\n",
        "  #\n",
        "  if i == 0:\n",
        "    adver_example_by_JSMA_t = cur_adver_example_by_JSMA_t\n",
        "  else:\n",
        "    adver_example_by_JSMA_t = torch.cat((adver_example_by_JSMA_t , cur_adver_example_by_JSMA_t), dim = 0)\n",
        "\n",
        "print(adver_example_by_JSMA_t.shape)\n",
        "print(clean_example_t.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXBxRsbq9Ng6",
        "outputId": "bc9326f5-adc3-4008-eb56-7641e74c510a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-3d63487e6aaa>:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  var_sample = Variable(torch.tensor(new_sample), requires_grad=True ).to(device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1000, 1, 28, 28])\n",
            "torch.Size([1000, 1, 28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# shuffle data\n",
        "# train data\n",
        "import random\n",
        "new_train_data = torch.cat((clean_example_f.to(device), adver_example_by_JSMA_f), dim=0)\n",
        "new_target_data = torch.cat((clean_target_f, adver_target_f), dim=0)\n",
        "batch_s = np.random.choice(len(clean_example_f) + len(adver_example_by_JSMA_f), len(clean_example_f) + len(adver_example_by_JSMA_f))\n",
        "new_train_data = new_train_data[batch_s]\n",
        "new_target_data = new_target_data[batch_s]\n",
        "\n",
        "print(new_train_data.shape)\n",
        "print(new_target_data.shape)\n",
        "print(new_target_data[0].dtype)\n",
        "\n",
        "# test data\n",
        "new_val_data = torch.cat((clean_example_t.to(device), adver_example_by_JSMA_t), dim=0)\n",
        "new_val_target_data = torch.cat((clean_target_t, adver_target_t), dim=0)\n",
        "batch_sv = np.random.choice(len(clean_example_t) + len(clean_example_t), len(clean_example_t) + len(clean_example_t))\n",
        "new_val_data = new_train_data[batch_sv]\n",
        "new_val_target_data = new_target_data[batch_sv]\n",
        "print(new_val_data.shape)\n",
        "print(new_val_target_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AFUve-59PPB",
        "outputId": "bc8590ab-76af-475f-d8b5-a895a92f73ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10000, 1, 28, 28])\n",
            "torch.Size([10000])\n",
            "torch.float32\n",
            "torch.Size([2000, 1, 28, 28])\n",
            "torch.Size([2000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# other model classifier\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        self.fc1 = nn.Linear(256, 120)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(84, 1)\n",
        "        self.relu5 = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.conv1(x)\n",
        "        y = self.relu1(y)\n",
        "        y = self.pool1(y)\n",
        "        y = self.conv2(y)\n",
        "        y = self.relu2(y)\n",
        "        y = self.pool2(y)\n",
        "        y = torch.flatten(y, 1)\n",
        "        y = self.fc1(y)\n",
        "        y = self.relu3(y)\n",
        "        y = self.fc2(y)\n",
        "        y = self.relu4(y)\n",
        "        y = self.fc3(y)\n",
        "        y = self.relu5(y)\n",
        "        return y"
      ],
      "metadata": {
        "id": "mkdA8jYP9RCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "921UIxHYKdZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# adver filter\n",
        "model_adv = Model().to(device)"
      ],
      "metadata": {
        "id": "RBnSHZDX9SxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# adver filter model training\n",
        "loss_fn = nn.BCELoss()\n",
        "epoch_c = 100\n",
        "def train_c(model,optimizer):\n",
        "  for i in range(epoch_c):\n",
        "    batch = np.random.choice(len(new_train_data), 200)\n",
        "    for j, data in enumerate(new_train_data[batch]):\n",
        "      model.train()\n",
        "      data = data.unsqueeze(0).to(device)\n",
        "      target = new_target_data[j].unsqueeze(0).unsqueeze(1)\n",
        "      # print(data.shape)\n",
        "      # print(target.shape)\n",
        "      # print(data)\n",
        "      # print(target)\n",
        "      logit = model(data)\n",
        "      loss = loss_fn(logit,target)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    if i % 10 == 0:\n",
        "      print ('第{}個epoch，loss值等於{}'.format(i,loss))"
      ],
      "metadata": {
        "id": "v0TsMFX39URV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_c(model_adv, optimizer2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p85p9WTKI_uw",
        "outputId": "d96544a8-8897-4388-bb36-d79de0759f57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "第0個epoch，loss值等於2.844424247741699\n",
            "第10個epoch，loss值等於2.8650243282318115\n",
            "第20個epoch，loss值等於3.115415573120117\n",
            "第30個epoch，loss值等於2.9540939331054688\n",
            "第40個epoch，loss值等於2.975090265274048\n",
            "第50個epoch，loss值等於2.8946244716644287\n",
            "第60個epoch，loss值等於2.9866344928741455\n",
            "第70個epoch，loss值等於2.931440830230713\n",
            "第80個epoch，loss值等於3.0564801692962646\n",
            "第90個epoch，loss值等於2.8399658203125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# validation of new lenet5 adver filter model\n",
        "\n",
        "def test_c(model,name):\n",
        "  model.eval()\n",
        "  correct_num = torch.tensor(0).to(device)\n",
        "  with torch.no_grad():\n",
        "    for j, data in tqdm(enumerate(new_val_data)):\n",
        "      data = data.unsqueeze(0).to(device)\n",
        "      target = new_val_target_data[j].unsqueeze(0).unsqueeze(1)\n",
        "      logit = model(data)\n",
        "      # print(logit)\n",
        "      pred = 1 if logit > 0.5 else 0\n",
        "      # pred = logit.max(1)[1].item()\n",
        "      # acc = logit.eq(target).sum().item() / 2000\n",
        "      # num = torch.sum(pred==target)\n",
        "      num = torch.sum(pred == target)\n",
        "      correct_num = correct_num + num\n",
        "    # print (correct_num)\n",
        "    # print ('\\n{} correct rate is {}'.format(name, acc))\n",
        "    print ('\\n{} correct rate is {}'.format(name, correct_num / 2000))"
      ],
      "metadata": {
        "id": "HETbzRU_9Y_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_c(model_adv, \"model_res accuacy\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxF0qWpn9a24",
        "outputId": "970f4079-011a-4afb-d3f6-7f1a819b4e3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2000it [00:01, 1634.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "model_res accuacy correct rate is 0.5215000510215759\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SL3hpmENU2SW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}